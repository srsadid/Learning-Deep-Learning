{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing data and pre processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2002 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# path \n",
    "train_data = \"F:\\\\pneumonia\\\\[FreeTutorials.Us] Udemy - Deep Learning Computer Vision\\\\10. Data Augmentation Build a Cats vs Dogs Classifier\\\\2.1 datasets.zip\\\\datasets\\\\catsvsdogs\\\\images\\\\train\"\n",
    "test_data = \"F:\\\\pneumonia\\\\[FreeTutorials.Us] Udemy - Deep Learning Computer Vision\\\\10. Data Augmentation Build a Cats vs Dogs Classifier\\\\2.1 datasets.zip\\\\datasets\\\\catsvsdogs\\\\images\\\\test\"\n",
    "\n",
    "#class and sizes of images and bacthes \n",
    "num_classes = 2\n",
    "img_rows, img_cols = 224, 224\n",
    "batch_size = 16\n",
    "\n",
    "## data augmentation\n",
    "\n",
    "# what we want \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      width_shift_range=0.3,\n",
    "      height_shift_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# applying on the data \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data,\n",
    "        #color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        test_data,\n",
    "        #color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_69 (Conv2D)           (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 224, 224, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 224, 224, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 224, 224, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 224, 224, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 224, 224, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 56, 56, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 28, 28, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 28, 28, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                3211328   \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,392,226\n",
      "Trainable params: 4,390,050\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 3)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 3)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call back, checkponts  and training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " 71/125 [================>.............] - ETA: 3:03 - loss: 1.0689 - accuracy: 0.5178"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-dd70007d39ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_validation_samples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     )\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"F:\\\\pneumonia\\\\[FreeTutorials.Us] Udemy - Deep Learning Computer Vision\\\\10. Data Augmentation Build a Cats vs Dogs Classifier\\\\2.1 datasets.zip\\\\datasets\\\\catsvsdogs\\\\images\\\\model_01.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 1000\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size\n",
    "    )\n",
    "\n",
    "import pickle \n",
    "pickle_out = open(\"checkpoint.pickle\" , 'wb')\n",
    "pickle.dump (checkpoint , pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "Confusion Matrix\n",
      "[[ 87 413]\n",
      " [ 43 457]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.67      0.17      0.28       500\n",
      "         dog       0.53      0.91      0.67       500\n",
      "\n",
      "    accuracy                           0.54      1000\n",
      "   macro avg       0.60      0.54      0.47      1000\n",
      "weighted avg       0.60      0.54      0.47      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import pickle \n",
    "\n",
    "# loading model\n",
    "pickle_in = open(\"checkpoint.pickle\" , 'rb')\n",
    "model = pickle.load (pickle_in)\n",
    "\n",
    "img_row, img_height, img_depth = 32,32,3\n",
    "model = load_model('F:\\\\pneumonia\\\\[FreeTutorials.Us] Udemy - Deep Learning Computer Vision\\\\10. Data Augmentation Build a Cats vs Dogs Classifier\\\\2.1 datasets.zip\\\\datasets\\\\catsvsdogs\\\\images\\\\model_01.h5')\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        test_data,\n",
    "        #color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#nb_train_samples = 2000\n",
    "#nb_validation_samples = 1000\n",
    "\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading model and getting class labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "{0: 'cat', 1: 'dog'}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import pickle \n",
    "\n",
    "\n",
    "pickle_in = open(\"checkpoint.pickle\" , 'rb')\n",
    "model = pickle.load (pickle_in)\n",
    "\n",
    "img_row, img_height, img_depth = 32,32,3\n",
    "model = load_model('F:\\\\pneumonia\\\\[FreeTutorials.Us] Udemy - Deep Learning Computer Vision\\\\10. Data Augmentation Build a Cats vs Dogs Classifier\\\\2.1 datasets.zip\\\\datasets\\\\catsvsdogs\\\\images\\\\model_01.h5')\n",
    "\n",
    "\n",
    "## getting our class labels \n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        test_data,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 500 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height)), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 32, 32\n",
    "\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "# predicting images\n",
    "\n",
    "test_data = \"F:\\\\pneumonia\\\\[FreeTutorials.Us] Udemy - Deep Learning Computer Vision\\\\10. Data Augmentation Build a Cats vs Dogs Classifier\\\\2.1 datasets.zip\\\\datasets\\\\catsvsdogs\\\\images\\\\test\\\\\"\n",
    "\n",
    "\n",
    "for i in range(0, 10):\n",
    "    path = test_data \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
